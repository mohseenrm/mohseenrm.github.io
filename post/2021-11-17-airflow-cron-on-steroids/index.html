<html><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=icon href=/images/favicon.svg><link rel=stylesheet href=/scss/global.min.c6e799be450052ebb6c2236c7a484e7dfc60256a0af456219d997f7d12dfe91e.css><link rel=stylesheet href=/css/prism.css><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css?family=Merriweather&display=swap" rel=stylesheet><link href="https://fonts.googleapis.com/css2?family=Fira+Code:wght@300;400;500;600;700&display=swap" rel=stylesheet><link href="https://fonts.googleapis.com/css2?family=Nunito+Sans:ital,wght@0,200;0,300;0,400;0,600;0,700;0,800;0,900;1,200;1,300;1,400;1,600;1,700;1,800;1,900&display=swap" rel=stylesheet><link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap" rel=stylesheet><link rel=stylesheet href=https://use.typekit.net/yka3uyp.css><title>Airflow Cron on Steroids | Mohseen</title><meta name=description content="Airflow
Airflow is a popular open-source framework for running data pipelines, but also really great at running tasks/scripts on a schedule. Cron has supported scheduling tasks for a while, but it&rsquo;s really hard to configure and monitor it correctly. Fortunately for me, Airflow was a lot easier to setup DAGs (Directed Acyclic Graphs) and dependencies needed between tasks.
Overview
Airflow has a few key features that make it easy to use:"><meta property="og:title" content="Airflow Cron on Steroids | Mohseen"><meta property="og:site_name" content="Mohseen"><meta property="og:description" content="Airflow
Airflow is a popular open-source framework for running data pipelines, but also really great at running tasks/scripts on a schedule. Cron has supported scheduling tasks for a while, but it&rsquo;s really hard to configure and monitor it correctly. Fortunately for me, Airflow was a lot easier to setup DAGs (Directed Acyclic Graphs) and dependencies needed between tasks.
Overview
Airflow has a few key features that make it easy to use:"><meta property="og:url" content="https://www.mohseen.dev/post/2021-11-17-airflow-cron-on-steroids/"><meta property="og:type" content="website"><meta property="og:locale" content="en_US"><meta property="og:image" content><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="Airflow Cron on Steroids | Mohseen"><link rel=canonical href=https://www.mohseen.dev/post/2021-11-17-airflow-cron-on-steroids/><meta name=twitter:description content="Airflow
Airflow is a popular open-source framework for running data pipelines, but also really great at running tasks/scripts on a schedule. Cron has supported scheduling tasks for a while, but it&rsquo;s really hard to configure and monitor it correctly. Fortunately for me, Airflow was a lot easier to setup DAGs (Directed Acyclic Graphs) and dependencies needed between tasks.
Overview
Airflow has a few key features that make it easy to use:"><meta name=twitter:image content><meta property="article:published_time" content="2021-11-17T11:25:55-08:00"><meta property="article:updated_time" content="2021-11-17T11:25:55-08:00"></head><body class=line-numbers><script src=/js/initColors.js></script><div class=layout-styled><section class=section><div class=nav-container><a class=logo-link href=/><div class=logo><svg viewBox="0 0 767.42 894.15"><path id="first-m" fill="#1ee3cf" d="m301.64 485.72-85.91-166.86v122.29l83.17 167.52c9.36 18.77 36.24 18.64 45.4-.25l9-18.58zm282.07 44.29V798.7a25.27 25.27.0 1050.54.0V425.52zM625.08 9.15a31.28 31.28.0 00-50 8L385.9 393.31l32.2 62.5 155.1-320.6a5.54 5.54.0 0110.53 2.42v128.4l50.54-100.47V31.22a30.86 30.86.0 00-9.19-22.07zM60.33 16.91A31.29 31.29.0 0032.52.0h-1.24A31.26 31.26.0 000 31.22V798.7a25.3 25.3.0 0025.27 25.27h1a25.3 25.3.0 0025.27-25.27V133.97a5.55 5.55.0 0110.53-2.41l40.08 80.77V101.39c0-1 0-2 .06-3z"/><path id="second-m" fill="#d1d3d4" d="M767.42 101.42v767.46a25.27 25.27.0 01-25.27 25.27 25.26 25.26.0 01-25.25-25.27V207.82a5.55 5.55.0 00-10.55-2.41L477.4 678.6c-9.15 18.91-36 19-45.38.23L195.27 201.72a5.55 5.55.0 00-10.52 2.43v664.7a25.27 25.27.0 01-25.27 25.3h-1a25.27 25.27.0 01-25.27-25.27V101.42a31.26 31.26.0 0131.23-31.27h1.24a31.27 31.27.0 0127.8 17l254.77 494.87a6.3 6.3.0 0011.22.0l248.76-494.6a31.26 31.26.0 0127.93-17.27 31.26 31.26.0 0131.26 31.27z"/></svg></div><span class=header-hidden>Navigate back to the homepage</span></a><div class=nav-controls><button id=copyButton class=icon-wrapper>
<svg class="icon-image" width="24" height="20" viewBox="0 0 24 20" fill="none"><path fillRule="evenodd" clipRule="evenodd" d="M2 5C2 3.34328 3.34328 2 5 2h9c1.6567.0 3 1.34328 3 3V9c0 1.6567-1.3433 3-3 3H10C9.44771 12 9 12.4477 9 13S9.44771 14 10 14h4c2.7613.0 5-2.2387 5-5V5c0-2.76128-2.2387-5-5-5H5C2.23872.0.0 2.23872.0 5V9c0 1.4938.656313 2.8361 1.6935 3.7509C2.10768 13.1163 2.73961 13.0767 3.10494 12.6625 3.47028 12.2483 3.43068 11.6164 3.0165 11.2511 2.39169 10.6999 2 9.89621 2 9V5zm5 6c0-1.65672 1.34328-3 3-3h4C14.5523 8 15 7.55228 15 7S14.5523 6 14 6H10C7.23872 6 5 8.23872 5 11v4c0 2.7613 2.23872 5 5 5h9c2.7613.0 5-2.2387 5-5V11C24 9.50621 23.3437 8.16393 22.3065 7.24906 21.8923 6.88372 21.2604 6.92332 20.8951 7.3375 20.5297 7.75168 20.5693 8.38361 20.9835 8.74894 21.6083 9.30007 22 10.1038 22 11v4c0 1.6567-1.3433 3-3 3H10c-1.65672.0-3-1.3433-3-3V11z" fill="#000"/></svg><div id=toolTip class=tool-tip>copied</div><input id=copyText style=opacity:0 type=text class=tool-tip>
</button>
<button id=themeColorButton class=icon-wrapper><div id=sunRays class=sun-rays></div><div id=moonOrSun class=moon-or-sun></div><div id=moonMask class=moon-mask></div></button></div></div></section><script src=/js/toggleLogos.js></script><script src=/js/toggleColors.js></script><script src=/js/copyUrl.js></script><section class="section narrow"><section id=articleHero class="section narrow"><div class=article-hero><header class=article-header><h1 class=article-hero-heading>Airflow Cron on Steroids</h1><div class=article-hero-subtitle><div class=article-meta><a href=/authors/mohseen-mukaddam/ class=article-author-link><div class=article-author-avatar><img src=/images/mohseen-mukaddam.jpg></div><strong>Mohseen Mukaddam</strong>
<span class=hide-on-mobile>,&nbsp;</span>
</a><script src=/js/collapseAuthors.js></script>November 17, 2021
â€¢ 7 min read</div></div></header><div class=article-hero-image id=ArticleImage__Hero><picture><source srcset=/images/hero/airflow-cron-on-steroids.jpg media="(min-width: 1280px) and (-webkit-min-device-pixel-ratio: 2), (min-width: 1280px) and (min-resolution: 192dpi)"><source srcset=/images/hero/airflow-cron-on-steroids.jpg media="(min-width: 1280px)"><source srcset=/images/hero/airflow-cron-on-steroids-tablet.jpg media="(min-width: 735px) and (-webkit-min-device-pixel-ratio: 2), (min-width: 735px) and (min-resolution: 192dpi)"><source srcset=/images/hero/airflow-cron-on-steroids-tablet.jpg media="(min-width: 735px)"><source srcset=818x434.jpg media="(min-width: 540px) and (-webkit-min-device-pixel-ratio: 2), (min-width: 540px) and (min-resolution: 192dpi)"><source srcset=/images/hero/airflow-cron-on-steroids-tablet.jpg media="(min-width: 540px)"><source srcset=/images/hero/airflow-cron-on-steroids-mobile.jpg media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 2), (min-width: 320px) and (min-resolution: 192dpi)"><source srcset=/images/hero/airflow-cron-on-steroids-mobile.jpg media="(min-width: 320px)"><source srcset=/images/hero/airflow-cron-on-steroids-mobile.jpg media="(min-width: 0px) and (-webkit-min-device-pixel-ratio: 2), (min-width: 0px) and (min-resolution: 192dpi)"><source srcset=/images/hero/airflow-cron-on-steroids-mobile.jpg media="(min-width: 0px)"><img src=/images/hero/airflow-cron-on-steroids.jpg alt></picture></div></div></section><aside id=progressBar class=aside-container><div class=aside-align><div><div class=overlap-container></div></div></div><div class=progress-container tabindex={-1}><div class=track-line aria-hidden=true><div id=progressIndicator class=progress-line></div></div></div></aside><article id=articleContent class=post-content style=position:relative><h2 id=airflow>Airflow</h2><p>Airflow is a popular open-source framework for running data pipelines, but also really great at running tasks/scripts on a schedule. Cron has supported scheduling tasks for a while, but it&rsquo;s really hard to configure and monitor it correctly. Fortunately for me, Airflow was a lot easier to setup DAGs (Directed Acyclic Graphs) and dependencies needed between tasks.</p><h3 id=overview>Overview</h3><p>Airflow has a few key features that make it easy to use:</p><ul><li><a href=https://airflow.apache.org/docs/apache-airflow/stable/concepts/scheduler.html>Scheduler</a> (helps in scheduling tasks once dependencies complete and fetch results)</li><li><a href=https://airflow.apache.org/docs/apache-airflow/stable/executor/index.html>Executors</a> (determines how to run the task, for local instances or remote instances)</li><li><a href=https://airflow.apache.org/docs/apache-airflow/stable/ui.html>Webserver</a> (allows you to view results, debug and trigger DAGs)</li><li><a href=https://airflow.apache.org/docs/apache-airflow/stable/howto/set-up-database.html>Metadata database</a> (stores all the state related informFation about DAGs)</li></ul><p><img src=/images/airflow-setup.jpg alt="Airflow Setup"></p><h3 id=installation>Installation</h3><p>While Airflow is designed to handle many tasks and process data pipelines, it&rsquo;s also a great substitute for running tasks locally or on a host, and makes it easier to manage and trigger DAGs.</p><pre tabindex=0><code class=language-sample data-lang=sample>AIRFLOW_VERSION=2.2.2
PYTHON_VERSION=&#34;$(python --version | cut -d &#34; &#34; -f 2 | cut -d &#34;.&#34; -f 1-2)&#34;
CONSTRAINT_URL=&#34;https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt&#34;
pip install &#34;apache-airflow[celery,postgres]==${AIRFLOW_VERSION}&#34; --constraint &#34;${CONSTRAINT_URL}&#34;
</code></pre><p>With <span class=highlight-block>Airflow 2</span>, they recently introduced a <a href=https://airflow.apache.org/docs/apache-airflow/stable/installation/installing-from-pypi.html#constraints-files>constraints version</a> that have known to be working versions of dependencies for reliable builds.</p><p>After first installing Airflow, it automatically creates a metadata database using SQLite. Only <span class=highlight-block>SequentialExecutor</span> is <a href=https://airflow.apache.org/docs/apache-airflow/stable/executor/index.html#executor-types>supported with SQLite</a>, so you can&rsquo;t use CeleryExecutor or any of the other supported Executors. Good news is, Airflow uses <span class=highlight-block>SQLAlchemy</span> for connecting to the database, so any database supported by it should be ready to be hooked up with Airflow. <a href=https://airflow.apache.org/docs/apache-airflow/stable/howto/set-up-database.html>Set up a database backend for airflow</a></p><p>Create a new database for airflow and grant permissions for accessing it. (using <span class=highlight-block>Postgres</span> for the example)</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CREATE</span> <span style=color:#66d9ef>DATABASE</span> airflow_db;
</span></span><span style=display:flex><span><span style=color:#66d9ef>CREATE</span> <span style=color:#66d9ef>USER</span> airflow_user <span style=color:#66d9ef>WITH</span> PASSWORD <span style=color:#e6db74>&#39;airflow_pass&#39;</span>;
</span></span><span style=display:flex><span><span style=color:#66d9ef>GRANT</span> <span style=color:#66d9ef>ALL</span> <span style=color:#66d9ef>PRIVILEGES</span> <span style=color:#66d9ef>ON</span> <span style=color:#66d9ef>DATABASE</span> airflow_db <span style=color:#66d9ef>TO</span> airflow_user;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>ALTER</span> <span style=color:#66d9ef>USER</span> airflow_user <span style=color:#66d9ef>SET</span> search_path <span style=color:#f92672>=</span> <span style=color:#66d9ef>public</span>;
</span></span></code></pre></div><p>Once you&rsquo;re all setup you can run the scheduler and webserver to see the predefined DAGs.</p><pre tabindex=0><code class=language-sample data-lang=sample>airflow webserver --port 8080
</code></pre><pre tabindex=0><code class=language-sample data-lang=sample>airflow scheduler
</code></pre><blockquote><p>Note: Only SequentialExecutor is supported with SQLite, so you canâ€™t use CeleryExecutor or any of the other supported Executors with the default MetaDB.</p></blockquote><p>One of the first changes I recommend is after using a different database update the executor type used from <span class=highlight-block>SequentialExecutor</span> to <span class=highlight-block>CeleryExecutor</span> or <span class=highlight-block>LocalExecutor</span>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-toml data-lang=toml><span style=display:flex><span><span style=color:#75715e># The executor class that airflow-runner should use. Choices include</span>
</span></span><span style=display:flex><span><span style=color:#75715e># ``SequentialExecutor``, ``LocalExecutor``, ``CeleryExecutor``, ``DaskExecutor``,</span>
</span></span><span style=display:flex><span><span style=color:#75715e># ``KubernetesExecutor``, ``CeleryKubernetesExecutor`` or the</span>
</span></span><span style=display:flex><span><span style=color:#75715e># full import path to the class when using a custom executor.</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>executor</span> = <span style=color:#a6e22e>CeleryExecutor</span>
</span></span></code></pre></div><h3 id=dags>DAGs</h3><p>The really neat feature about Airflow is you can create tasks in form of DAGs that let you sequence tasks and dependencies. So if you wanted to fetch information check
if any new records are present and sync with database or external services like S3 it&rsquo;s easy to configure.</p><div class=Image__Medium><img src=/images/What-is-a-directed-acyclic-graph.png alt="Directed Acyclic Graph"></div><p>DAGs also have CRON presets with options to catch up with missed tasks, backfill on a time interval, and retries on failure.</p><p>Here&rsquo;s an example of setting a few bash scripts in sequence.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> datetime <span style=color:#f92672>import</span> timedelta
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> airflow <span style=color:#f92672>import</span> DAG
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> airflow.operators.bash <span style=color:#f92672>import</span> BashOperator
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> airflow.operators.dummy <span style=color:#f92672>import</span> DummyOperator
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> airflow.utils.dates <span style=color:#f92672>import</span> days_ago
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>args <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;owner&#39;</span>: <span style=color:#e6db74>&#39;airflow&#39;</span>,
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>dag <span style=color:#f92672>=</span> DAG(
</span></span><span style=display:flex><span>    dag_id<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;example_bash_operator&#39;</span>,
</span></span><span style=display:flex><span>    default_args<span style=color:#f92672>=</span>args,
</span></span><span style=display:flex><span>    schedule_interval<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;0 0 * * *&#39;</span>,
</span></span><span style=display:flex><span>    start_date<span style=color:#f92672>=</span>days_ago(<span style=color:#ae81ff>2</span>),
</span></span><span style=display:flex><span>    dagrun_timeout<span style=color:#f92672>=</span>timedelta(minutes<span style=color:#f92672>=</span><span style=color:#ae81ff>60</span>),
</span></span><span style=display:flex><span>    tags<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;example&#39;</span>, <span style=color:#e6db74>&#39;example2&#39;</span>],
</span></span><span style=display:flex><span>    params<span style=color:#f92672>=</span>{<span style=color:#e6db74>&#34;example_key&#34;</span>: <span style=color:#e6db74>&#34;example_value&#34;</span>},
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>run_this_last <span style=color:#f92672>=</span> DummyOperator(
</span></span><span style=display:flex><span>    task_id<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;run_this_last&#39;</span>,
</span></span><span style=display:flex><span>    dag<span style=color:#f92672>=</span>dag,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>run_this_first <span style=color:#f92672>=</span> BashOperator(
</span></span><span style=display:flex><span>    task_id<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;run_this_first&#39;</span>,
</span></span><span style=display:flex><span>    bash_command<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;echo Hola Mundo&#39;</span>,
</span></span><span style=display:flex><span>    dag<span style=color:#f92672>=</span>dag,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>run_this_first <span style=color:#f92672>&gt;&gt;</span> run_this_last
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    dag<span style=color:#f92672>.</span>cli()
</span></span></code></pre></div><p>The operators let you define tasks that can be linked to a single DAG.</p><p><img src=/images/example_dag_1.png alt=example_dag_1></p><p>Tasks/Operators can be dynamically created and linked to different DAGs which in turn can be reused across different DAG flows.</p><p>You can create new tasks based on dynamic results like fetch account history for a list of users, letting you run the same task for different users. Modifying the above example to setup tasks dynamically. Example from Airflow docs</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>run_this_last <span style=color:#f92672>=</span> DummyOperator(
</span></span><span style=display:flex><span>    task_id<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;run_this_last&#39;</span>,
</span></span><span style=display:flex><span>    dag<span style=color:#f92672>=</span>dag,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>run_this <span style=color:#f92672>=</span> BashOperator(
</span></span><span style=display:flex><span>    task_id<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;run_after_loop&#39;</span>,
</span></span><span style=display:flex><span>    bash_command<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;echo Hola Mundo&#39;</span>,
</span></span><span style=display:flex><span>    dag<span style=color:#f92672>=</span>dag,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>run_this <span style=color:#f92672>&gt;&gt;</span> run_this_last
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>3</span>):
</span></span><span style=display:flex><span>    task <span style=color:#f92672>=</span> BashOperator(
</span></span><span style=display:flex><span>        task_id<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;runme_&#39;</span> <span style=color:#f92672>+</span> str(i),
</span></span><span style=display:flex><span>        bash_command<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;echo &#34;{{ task_instance_key_str }}&#34; &amp;&amp; sleep 1&#39;</span>,
</span></span><span style=display:flex><span>        dag<span style=color:#f92672>=</span>dag,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    task <span style=color:#f92672>&gt;&gt;</span> run_this
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>also_run_this <span style=color:#f92672>=</span> BashOperator(
</span></span><span style=display:flex><span>    task_id<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;also_run_this&#39;</span>,
</span></span><span style=display:flex><span>    bash_command<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;echo &#34;run_id={{ run_id }} | dag_run={{ dag_run }}&#34;&#39;</span>,
</span></span><span style=display:flex><span>    dag<span style=color:#f92672>=</span>dag,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>also_run_this <span style=color:#f92672>&gt;&gt;</span> run_this_last
</span></span></code></pre></div><p><img src=/images/example_dag_2.png alt=example_dag_2></p><h3 id=operators>Operators</h3><p>Operators in Airflow are tasks with baked in functionality, that makes it easier to setup and configure tasks. If you have a lot of scripts written in Python, it can be
easier to pull into Airflow and run it via <span class=highlight-block>PythonOperator</span> or <span class=highlight-block>PythonVirtualenvOperator</span>.</p><p>Operators can define tasks that run a bash script, or write records to a database or send an email. Here&rsquo;s a <a href=https://airflow.apache.org/docs/apache-airflow/stable/operators-and-hooks-ref.html>list of core operators</a> supported in Airflow.</p><p>There are also <a href=https://airflow.apache.org/docs/apache-airflow-providers/index.html>community provider packages</a> for Postges, S3, Docker, etc. that allows you to interact with other services while building your DAGs. Example of using <span class=highlight-block>PythonOperator</span> to setup a DAG for fetching and updating OHLC data for a list of stocks/crypto.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>update_crypto_history</span>(ticker: str):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;Gets current history and updates records in database&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    session <span style=color:#f92672>=</span> get_db_session()
</span></span><span style=display:flex><span>    ticker_last_updated <span style=color:#f92672>=</span> (
</span></span><span style=display:flex><span>        session<span style=color:#f92672>.</span>query(StockHistory)
</span></span><span style=display:flex><span>            <span style=color:#f92672>.</span>filter(StockHistory<span style=color:#f92672>.</span>ticker <span style=color:#f92672>==</span> ticker)
</span></span><span style=display:flex><span>            <span style=color:#f92672>.</span>order_by(StockHistory<span style=color:#f92672>.</span>quote_date<span style=color:#f92672>.</span>desc())
</span></span><span style=display:flex><span>            <span style=color:#f92672>.</span>first()
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    results <span style=color:#f92672>=</span> YahooClient()<span style=color:#f92672>.</span>history(ticker, crypto<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> ticker_last_updated:
</span></span><span style=display:flex><span>        next_date <span style=color:#f92672>=</span> ticker_last_updated<span style=color:#f92672>.</span>quote_date <span style=color:#f92672>+</span> datetime<span style=color:#f92672>.</span>timedelta(days<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        results <span style=color:#f92672>=</span> results[next_date<span style=color:#f92672>.</span>strftime(DATE_TIME_FORMAT):]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> len(results) <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>        db_objs <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>            StockHistory(
</span></span><span style=display:flex><span>                ticker<span style=color:#f92672>=</span>ticker,
</span></span><span style=display:flex><span>                quote_date<span style=color:#f92672>=</span>row[<span style=color:#e6db74>&#34;Date&#34;</span>],
</span></span><span style=display:flex><span>                open<span style=color:#f92672>=</span>row[<span style=color:#e6db74>&#34;Open&#34;</span>],
</span></span><span style=display:flex><span>                high<span style=color:#f92672>=</span>row[<span style=color:#e6db74>&#34;High&#34;</span>],
</span></span><span style=display:flex><span>                low<span style=color:#f92672>=</span>row[<span style=color:#e6db74>&#34;Low&#34;</span>],
</span></span><span style=display:flex><span>                last<span style=color:#f92672>=</span>row[<span style=color:#e6db74>&#34;Close&#34;</span>],
</span></span><span style=display:flex><span>                volume<span style=color:#f92672>=</span>row[<span style=color:#e6db74>&#34;Volume&#34;</span>],
</span></span><span style=display:flex><span>            )
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> (_, row) <span style=color:#f92672>in</span> results<span style=color:#f92672>.</span>iterrows()
</span></span><span style=display:flex><span>        ]
</span></span><span style=display:flex><span>        records <span style=color:#f92672>=</span> len(db_objs)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;### Writing </span><span style=color:#e6db74>{</span>records<span style=color:#e6db74>}</span><span style=color:#e6db74> records for ticker: </span><span style=color:#e6db74>{</span>ticker<span style=color:#e6db74>}</span><span style=color:#e6db74> ###&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        session<span style=color:#f92672>.</span>add_all(db_objs)
</span></span><span style=display:flex><span>        session<span style=color:#f92672>.</span>commit()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;### Write complete for ticker: </span><span style=color:#e6db74>{</span>ticker<span style=color:#e6db74>}</span><span style=color:#e6db74>, records added: </span><span style=color:#e6db74>{</span>records<span style=color:#e6db74>}</span><span style=color:#e6db74> ###&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;No new records added for ticker: </span><span style=color:#e6db74>{</span>ticker<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>print_context</span>(ds, <span style=color:#f92672>**</span>kwargs):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;Print the Airflow context and ds variable from the context.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    pprint(kwargs)
</span></span><span style=display:flex><span>    print(ds)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#e6db74>&#34;Whatever you return gets printed in the logs&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>run_this <span style=color:#f92672>=</span> PythonOperator(
</span></span><span style=display:flex><span>    task_id<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;print_the_context&#34;</span>,
</span></span><span style=display:flex><span>    python_callable<span style=color:#f92672>=</span>print_context,
</span></span><span style=display:flex><span>    dag<span style=color:#f92672>=</span>dag,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>tickers <span style=color:#f92672>=</span> get_all_crypto_tickers_for_portfolio(SAMPLE_ID)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># setup update task for each ticker</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> ticker_symbol <span style=color:#f92672>in</span> tickers:
</span></span><span style=display:flex><span>    task <span style=color:#f92672>=</span> PythonOperator(
</span></span><span style=display:flex><span>        task_id<span style=color:#f92672>=</span><span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;update_crypto_history_</span><span style=color:#e6db74>{</span>ticker_symbol<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>,
</span></span><span style=display:flex><span>        python_callable<span style=color:#f92672>=</span>update_crypto_history,
</span></span><span style=display:flex><span>        op_kwargs<span style=color:#f92672>=</span>{<span style=color:#e6db74>&#34;ticker&#34;</span>: ticker_symbol},
</span></span><span style=display:flex><span>        dag<span style=color:#f92672>=</span>dag,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    run_this <span style=color:#f92672>&gt;&gt;</span> task
</span></span></code></pre></div><blockquote><p>Note: beware of database connections used inside DAGs as they are created for every DAG task run.</p></blockquote><p>It might be better to have them scoped to each operation that needs it, or gather them from a connection pool. Airflow recommends using <a href=https://www.pgbouncer.org/>Pgbouncer</a> to manage connections to <span class=highlight-block>Postgres</span> from DAGs. See <a href=https://airflow.apache.org/docs/apache-airflow/stable/howto/set-up-database.html>guide to setup a
database backend</a> for Airflow.</p><h3 id=triggering-dags>Triggering DAGs</h3><p>DAG runs can be triggered manually using cli / web UI or can be triggered by api once it&rsquo;s enabled with airflow. Generally DAG runs are triggered when it scheduled to
run, you can set the schedule for a DAG using <span class=highlight-block>schedule_interval</span> which uses a CRON format, although it&rsquo;s not strictly required to have one set in order to run DAGs.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>airflow dags trigger --conf <span style=color:#e6db74>&#39;{&#34;user&#34;: &#34;sample_id&#34;, &#34;action&#34;: &#34;send_password_update_notification&#34;}&#39;</span> dag_id
</span></span></code></pre></div><p>It&rsquo;s also possible to trigger DAG runs using the Airflow API, but you would need to enable it with the Airflow webserver to allow api requests. Check out <a href=/post/2021-11-30-enabling-airflow-api/>this article</a> on how to enable Airflow API for your Airflow webserver.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>curl -X POST http://airflow-service:<span style=color:#f92672>[</span>PORT_NUMBER<span style=color:#f92672>]</span>/api/v1/dags/<span style=color:#f92672>{</span>dag_id<span style=color:#f92672>}</span>/dagRuns <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    -H <span style=color:#e6db74>&#34;Content-Type: application/json&#34;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --user <span style=color:#e6db74>&#34;username:password&#34;</span>
</span></span></code></pre></div><h3 id=secret-manager-and-dockerizing-airflow>Secret manager and Dockerizing Airflow</h3><p>Airflow has a separate secrets backend that allows you to store secrets in a secure way. This is useful for storing connection strings, passwords, and managing different environments. These connections can be accessed from your DAGs and used in any operator that needs them. Note, when dockerizing you Airflow application, you might need
to sync the <span class=highlight-block>SQLAlchemy</span> connection string and <span class=highlight-block>Fernet</span> key used by Airflow.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#75715e># Environment Variables</span>
</span></span><span style=display:flex><span>AIRFLOW__CORE__SQL_ALCHEMY_CONN
</span></span><span style=display:flex><span>AIRFLOW__CORE__FERNET_KEY
</span></span><span style=display:flex><span><span style=color:#75715e># airflow.cfg</span>
</span></span><span style=display:flex><span>sql_alchemy_conn
</span></span><span style=display:flex><span>fernet_key
</span></span></code></pre></div><p>Airflow encrypts connections and variables stored using the fernet key, and will not be usable unless you use the same key to decrypt the secrets. <a href=https://airflow.apache.org/docs/apache-airflow/stable/security/secrets/fernet.html>More info</a> on using and managing fernet keys.</p><h3 id=cron-replacement>CRON replacement</h3><p>While Airflow was more designed for data pipelines and creating workflows, it&rsquo;s a great alternative to using CRON. Making it easier to schedule and trigger DAGs and catch up on any missed runs. It&rsquo;s more easily testable and has great support for connecting to different services making it one of my favorite for setting up new scripts / scraping information.</p><section id=subscriptionSection class="section narrow"><div class=subscription-container><div class=subscription-content><h3 class=subscription-heading>Join the email list and get notified about new content</h3><p class=subscription-text>Be the first to receive latest content with the ability to opt-out at
anytime.<br>We promise to not spam your inbox or share your email with any third
parties.</p><form id=subscriptionForm class=subscription-form action=https://formspree.io/f/mpzbwzjr method=POST><input id=emailInput class=subscription-input placeholder=your@email.com name=email type=email required pattern="^[a-zA-Z0-9.!#$%&'*+/=?^_`{|}~-]+@[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?(?:\.[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?)*$">
<button id=submitButton class=submit-button type=submit>
Subscribe</button><div class=subscription-error-message>The email you entered is not valid.</div></form></div></div></section><script src=/js/addFormStyles.js></script></article><section id=articleNext class="section nartrow"><h3 class=footer-next-heading>More articles from Mohseen</h3><div class=footer-spacer></div><div class=next-articles-grid numberofarticles={numberOfArticles}><div class=post-row><a href=/post/2021-11-15-bypass-ip-based-rate-limits/ class=article-link id=article-link-bigger><div><div class=image-container><img src=/images/hero/bypass-ip-based-rate-limits.jpg class=article-image></div><div><h2 class=article-title>Bypass IP based Rate Limits for Public APIs</h2><p class=article-excerpt>Guide to using free proxies to bypass IP based rate limits.</p><div class=article-metadata>November 15, 2021 Â· 7 min read</div></div></div></a><a href=/post/2021-11-12-using-rnn-for-predicting-closing-prices/ class=article-link><div><div class=image-container><img src=/images/hero/Using-RNN-for-Predicting-Closing-Prices.jpg class=article-image></div><div><h2 class=article-title>Using RNN for Predicting Closing Prices</h2><p class=article-excerpt>Experimenting with Recurrent Neural Networks for predicting closing prices for Bitcoin.</p><div class=article-metadata>November 12, 2021 Â· 8 min read</div></div></div></a></div></div></section></section><script src=/js/progressBar.js></script><div class=footer-gradient></div><div class="section narrow"><div class=footer-hr></div><div class=footer-container><div class=footer-text>Â© 2025 Mohseen</div><div class=social-icon-outer><div class=social-icon-container></div></div></div></div></div><script src=/js/prism.js></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-WBJ981VMT6"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-WBJ981VMT6")}</script></body></html>